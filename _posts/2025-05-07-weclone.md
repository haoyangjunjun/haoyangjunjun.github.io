---
layout:     post
title:      "weclone部署记录"
subtitle:   "尝试制作赛博好氧菌"
date:       2025-05-07 12:00:00
author:     "hangyangjun"
header-img: "img/weclone.png"
tags:
    - 笔记
    - 记录
    - 学习
    - AI
    - 微调
    - docker
    - AstrBot
    - NapCat
---

# day one
尝试制作赛博好氧菌  
微信和某好友的对话有足够的数据量

[weclone项目](https://github.com/xming521/WeClone?tab=readme-ov-file)  
记得之前搞过ai，cuda装过了

安装 [uv](https://docs.astral.sh/uv/)  
一个Python 包和项目经理

git克隆目录

powershell：  
`uv venv .venv --python=3.10`

因为用的windows环境，只能试试了

`.venv\Scripts\activate`  
**激活虚拟环境**

`uv pip install --group main -e .`  
安装  
还挺慢

不过这个多线程下载挺爽的

`python -c "import torch; print('CUDA是否可用:', torch.cuda.is_available());"`  

检测环境------true

安装 FlashAttention 加速训练和推理：uv pip install flash-attn --no-build-isolation

但是安装失败了

数据准备  
数据预处理  
这一部分稍后再搞  
先试试其他部分

模型下载
`git lfs install`
跟踪和管理大文件

`git clone https://www.modelscope.cn/Qwen/Qwen2.5-7B-Instruct.git`

下载模型也挺慢的，而且还没有进度条。。。。

好吧，我收回我的话，速度其实挺快的，而且竟然不走代理。。。


```
这些参数是一个典型的 AI 模型训练配置（尤其是针对大语言模型或类似任务的微调），通常用于控制训练过程的行为。以下是每个参数的详细解释：

基础训练配置
stage: "sft"
表示当前训练阶段是 Supervised Fine-Tuning (SFT)，即有监督微调。常见于指令微调（如 LLaMA、ChatGLM 等模型的指令跟随能力训练）。
dataset: "wechat-sft"
使用的数据集名称，这里可能是自定义的微信对话数据集或其他指令跟随数据集。
dataset_dir: "./dataset/res_csv/sft"
数据集存放的本地路径，通常包含训练样本（如 CSV、JSON 或文本文件）。
use_fast_tokenizer: true
是否使用快速分词器（如 Hugging Face 的 FastTokenizer），牺牲少量精度以加速分词过程。
LoRA 配置（低秩适配）
LoRA（Low-Rank Adaptation）是一种轻量级微调技术，通过低秩矩阵近似参数更新，减少计算量。

lora_target: "q_proj,v_proj"
指定在哪些网络层应用 LoRA。这里可能是注意力机制中的 Query 投影（q_proj） 和 Value 投影（v_proj） 层。
lora_rank: 4
LoRA 矩阵的秩（rank），值越小计算量越低，但可能牺牲表达能力。通常设为 4-64。
lora_dropout: 0.4
LoRA 层的 dropout 概率，防止过拟合（0.4 表示 40% 的神经元会被随机丢弃）。
优化器与正则化
weight_decay: 0.1
L2 正则化系数，惩罚大权重值以防止过拟合（0.1 是较强正则化）。
overwrite_cache: true
是否覆盖已缓存的预处理数据（如分词后的结果），设为 true 可强制重新处理数据。
训练批处理与梯度
per_device_train_batch_size: 8
每个 GPU/CPU 上的训练批次大小（batch size），影响内存占用和训练速度。
gradient_accumulation_steps: 4
梯度累积步数。模拟更大的批次大小（实际 batch_size = 8 × 4 = 32），缓解显存不足问题。
学习率调度
lr_scheduler_type: "cosine"
学习率调度器类型，cosine 表示余弦退火，学习率随训练过程平滑变化（先下降后回升）。
learning_rate: 1e-4
初始学习率（0.0001），常见于微调任务。
warmup_ratio: 0.1
学习率预热比例（前 10% 的训练步数中，学习率从 0 线性增长到 1e-4），避免初始震荡。
训练时长与日志
num_train_epochs: 3
训练轮数（3 轮遍历整个数据集）。
logging_steps: 10
每 10 步记录一次训练日志（如损失、学习率等）。
save_steps: 100
每 100 步保存一次模型检查点。
其他技术
cutoff_len: 256
文本截断长度（256 个 token），超长部分会被丢弃，影响上下文窗口大小。
plot_loss: true
是否绘制训练损失曲线（可视化工具如 TensorBoard 或 WandB）。
fp16: true
使用混合精度训练（FP16），减少显存占用并加速计算（需 GPU 支持）。
flash_attn: "fa2"
使用 Flash Attention 第 2 版（优化注意力计算的算法，显著提升速度并降低显存）。
总结
目标：通过 LoRA 轻量级微调一个语言模型，使其适应指令跟随任务（SFT 阶段）。
关键技术：LoRA、混合精度（FP16）、Flash Attention、余弦学习率调度。
适用场景：中等规模数据集（如微信对话数据），在单卡或多卡 GPU 上训练。
如果有特定参数需要进一步优化或解释，可以结合具体任务调整（如增大 lora_rank 或降低 lora_dropout）。
```

`python weclone/train/train_sft.py`  
尝试训练，但是突然发现没有数据，嘿嘿🤣

聊天记录迁移还能出bug，微信真垃圾还得想办法  
通过，关闭防火墙、禁用虚拟网卡、连接手机热点（宿舍WiFi有隔离）终于解决了  
但是速度巨慢无比。

终于是迁移完了，差点给我跑步耽误了。

聊天记录导出工具：[PyWxDump](https://github.com/xaoyaoo/PyWxDump)

导出的csv 文件夹放在./dataset目录

**数据处理**  
`python weclone/data/qa_generator.py`

数据就搞好了，接下来就可以训练了  
不过我这6G显存不知道能不能行呢

总之先试试：
**单卡训练**
`python weclone/train/train_sft.py`

好家伙，直接给显卡干满了
。。。。
然后就无了
报错了，研究研究

很明显爆显存了

`uv pip install bitsandbytes`

好像没用

改了setting又报错，还不熟悉这些训练框架之类的，也不知道能不能改成4bit精度

还是直接换个模型试试吧

*Qwen2.5-1.5B-Instruct*  
就挺好

`git clone https://www.modelscope.cn/Qwen/Qwen2.5-1.5B-Instruct.git`  

再改一下setting

还是不行
重新处理数据试试

重启试试

没用。。。。。。

重新下载一遍模型试试
ok 成功了，好像是之前下载的时候出问题了。。。。。

貌似是训练完了，但是收敛的好像不太好  
想再尝试，但是硬盘满了，而且训练时显存已经快爆了  

试试安装  
flash-attn总是失败  
cuda删了下了12.6版本的试试

还是不行，已经到第二天了，暂时放弃

**尝试使用浏览器demo简单推理**  
`python weclone/eval/web_demo.py`  
可以在这一步测试出合适的`temperature` `top_p`值，修改`settings.json`的`infer_args`后，供后续推理时使用。


终于运行出来了  
而且回复速度超级快

虽然很智障，但是也能算个加减乘除  
以及回复的语气有点意思，但是感觉也不是很像我

需要更多测试

```
Top-p采样值（Nucleus Sampling）

高创造性任务（如故事生成）：p=0.9 允许更多多样性。  
低容错任务（如代码生成）：p=0.3~0.5 限制随机性。  
极端值：  
p=1：等同于贪心搜索（生成最可能的结果，但可能重复）。  
p=0：随机选择（无意义）。


温度系数（Temperature）

对话系统：T=0.7~0.9 平衡合理性与多样性。  
诗歌生成：T=1.2~1.5 增加创造力。  
事实性问答：T=0.3~0.5 避免错误。  
极端值：  
T=0：完全选择概率最高的token（等同于贪心搜索）。  
T=∞：完全随机选择（无意义）。

需要确定性（如代码生成）：低温 + 低Top-p。  
需要多样性（如故事生成）：高温 + 高Top-p。  
```

测试：
>你好  
用go语言写helloworld  
今晚玩什么  
上号  
吃什么  
几点玩  
[其他测试题](https://docs.zetatechs.com/llm-test/#_4)

问了一堆问题，进行微调，感觉都不太聪明，也许是模型大小的限制吧，2：00了，白天再搞了，该碎觉了。

看了一眼，微信对话机器人的部署有点复杂，看来还得学一下docker

# day two
ok  
时隔一天，我又来搞了

我又找了一些聊天数据,打算重新训练一下(因为之前搞得这个不怎么好)  
快速的过一遍之前的流程  
结果最后爆显存了,明明只是多了不到200k的文本,最后降低参数   "per_device_train_batch_size"到4,就好了,刚刚好显存够用.  
最终训练损失train_loss = 1.3499，表明模型在训练数据上收敛正常。  
虽然测试还是很智障,还很叛逆 

然后考虑部署微信对话机器人  
必须使用docker  

先装一个[WSL2（Windows Subsystem for Linux 2）](https://learn.microsoft.com/zh-cn/windows/wsl/)的系统  
[参考教程](https://blog.csdn.net/Natsuago/article/details/145594631?spm=1001.2014.3001.5501)  

`wsl --list --online`
查看可用WSL 发行版  

`wsl --install -d Ubuntu-20.04`  
安装  
没走代理，速度不快  
系统就装在c盘吧，毕竟D盘剩余空间已经不如c盘了

设个用户名haoyangjun，再来个吊炸天的弱密码

发现子系统好像用不了我现在的代理......  
[有点难搞了](https://learn.microsoft.com/zh-cn/windows/wsl/networking#mirrored-mode-networking)

新增.wslconfig设置(开了一个镜像网络之类的设置-参照官方文档)
```   
[wsl2]  
networkingMode = mirrored  
dnsTunneling = true  
autoProxy = true  

wsl --shutdown  
wsl -d Ubuntu-20.04  
```
重启

然后代理开虚拟网卡模式
ping了一下谷歌，延迟很低，看来网络因该没问题了，那就先这样了

安装 [Docker](https://www.docker.com/)  

[参考教程(csdn)](https://blog.csdn.net/Natsuago/article/details/145588600)

安装，注册一气呵成
用于 Docker Desktop 的中文语言包，链接地址为：[DockerDesktop-CN](https://github.com/asxez/DockerDesktop-CN)

使用 Docker 部署 AstrBot  
读了一下文档，感觉使用挺简单，明天再搞，不能睡太晚

先克隆个仓库  
PS D:\git_ghrepo> `git clone https://github.com/AstrBotDevs/AstrBot`

在目录下运行  
PS D:\git_ghrepo\AstrBot> `docker compose up -d`

结果又卡在网络上了，不知道为什么代理没效果

搞了一堆设置，各种报错，最后关闭系统代理，关闭docker手动代理，用clash的虚拟网卡模式，速度还挺快

上一步骤完成后，输入  

`docker logs -f astrbot`  

可以看到  
WebUI的地址和默认用户密码    
访问即可进入控制面板  
至此，终于成功部署

做到这里才发现 Gewechat 在前几天已经停止维护了......

总之先试试

把 Gewechat 的镜像下下来之后因该是正常运行了  
用astrbot链接  
但是失败

又卡在这里

好吧，最后网络都配置好了，但是由于Gewechat停止维护了，即使本地装好了好像也不能登录......只能考虑别的方法了

可以试试qq
通过 [NapCatQQ](https://napneko.github.io/guide/boot/Shell#napcat-shell-win%E6%89%8B%E5%8A%A8%E5%90%AF%E5%8A%A8%E6%95%99%E7%A8%8B) 协议实现端接入 QQ

比微信方便多了  
下载完，直接运行`launcher.bat`就好了，然后可以打开webui

结果 NapCatQQ 中添加 WebSocket 客户端一直连不上，因该还是网络问题

重新试了一次好像成功了嘿嘿

用之前搞的qq小号，测试成功了，输入/help成功给我回复了

接下来就是连上大模型了

执行 `python weclone/server/api_service.py`  启动api服务

报错：ERROR:    [Errno 13] error while attempting to bind on address ('127.0.0.1', 8006): 以一种访问权限不允许的方式做了一个访问套接字的尝试。

找不到端口占用，换端口也不行，最后没办法竟然通过重启解决了，无语(ˉ▽ˉ；)...  

最后。。。。。还是出问题了  
在 AstrBot 中新增服务提供商连不上api

最后还是调成功了，因该是docker访问127.0.0.1不是宿主机的网的问题

还差最后一个问题，微调失效了

管理员输入  
`/tool off_all`

好像还是不行

始终是出不来微调后的效果

都重启了一下，突然发现好像有微调效果了，很奇怪，效果不如web demo的好，保留了更多ai的味道，也让朋友测试了一下，这ai说的狗屁不通

不过至此，对于这个项目的复刻（没想到更贴切的词）终于完成了，所有功能正常。美中不足的是我遇到了太多问题，好在最后都基本解决了，虽然部分不明白是什么原理，还有对ai训练的知识实在是不足，基本上没办法改动别人的代码，然后就是使用的几个软件，以及docker，掌握速度很慢，然后就是可恶的网络，对于自己天天用的代理都了解不深，还有debug时问ai结果一点用没有，看来ai取代程序员还需要写时日。  
最后，阅读官方文档是个好习惯（如果文档写的一团糟就没办法了），此次用时两个晚上+一个下午，最后搞完还是很有成就感的😀。